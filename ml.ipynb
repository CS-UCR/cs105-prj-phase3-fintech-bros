{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning to predict stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it up again\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from afinn import Afinn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about the different data models used for machine learning, hypos of sentiment analysis use in prediction. Compare top1 and top25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddit_news_df = pd.read_csv(\"Combined_News_DJIA.csv\")\n",
    "# reddit_news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in phase 2 we found out only 3 rows contain null values, so I'm dropping those again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_news_df = reddit_news_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question one. Are sentiment analysis values of the more popular headlines/articles in the r/worldnews subreddit, better stock market predictors than less popular article?\n",
    "In the following cells I'll train logistic regression models using sentiment analysis of different Top# articles to predict whether the Dow Jones Industrial Average(DJIA) goes up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = Afinn()\n",
    "\n",
    "top1_headlines = reddit_news_df['Top1']\n",
    "top1_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top1_headlines]\n",
    "top1_sentiment_scores = [af.score(headline) for headline in top1_headlines]\n",
    "\n",
    "reddit_news_df['Top1 Sentiment'] = top1_sentiment_scores\n",
    "\n",
    "top5_headlines = reddit_news_df['Top5']\n",
    "top5_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top5_headlines]\n",
    "top5_sentiment_scores = [af.score(headline) for headline in top5_headlines]\n",
    "\n",
    "reddit_news_df['Top5 Sentiment'] = top5_sentiment_scores\n",
    "\n",
    "top15_headlines = reddit_news_df['Top15']\n",
    "top15_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top15_headlines]\n",
    "top15_sentiment_scores = [af.score(headline) for headline in top15_headlines]\n",
    "\n",
    "reddit_news_df['Top15 Sentiment'] = top15_sentiment_scores\n",
    "\n",
    "top25_headlines = reddit_news_df['Top25']\n",
    "top25_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top25_headlines]\n",
    "top25_sentiment_scores = [af.score(headline) for headline in top25_headlines]\n",
    "\n",
    "reddit_news_df['Top25 Sentiment'] = top25_sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_train = reddit_news_df.loc[:994].copy()\n",
    "reddit_test = reddit_news_df.loc[944:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Top1 headlines sentiment analysis values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(top1_sentiment_scores))\n",
    "x_train = reddit_train[['Top1 Sentiment']]\n",
    "x_test = reddit_test[['Top1 Sentiment']]\n",
    "\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.53\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logReg.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negatives: 3\n",
      "True positives: 488\n",
      "False negatives: 3\n",
      "False positives: 551\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('True negatives:', tn)\n",
    "print('True positives:', fp)\n",
    "print('False negatives:', fn)\n",
    "print('False positives:', tp)\n",
    "# print(confusion_matrix)\n",
    "# tn = []\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.01      0.01       491\n",
      "          1       0.53      0.99      0.69       554\n",
      "\n",
      "avg / total       0.52      0.53      0.37      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Accuracy of model using Top1 article sentiment analysis values</b>\n",
    "\n",
    "As you can see above the accuracy of the model isn't great. A little over half of the DJIA values were predicted correctly.Furthermore, based off of the confusion matrix information, it seems the model is better at predicting when the DJIA is going to up or stay the same than predicting when its going to go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Top5 headlines sentiment analysis values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.53\n",
      "True negatives: 0\n",
      "True positives: 491\n",
      "False negatives: 0\n",
      "False positives: 554\n"
     ]
    }
   ],
   "source": [
    "x_train = reddit_train[['Top5 Sentiment']]\n",
    "x_test = reddit_test[['Top5 Sentiment']]\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('True negatives:', tn)\n",
    "print('True positives:', fp)\n",
    "print('False negatives:', fn)\n",
    "print('False positives:', tp)\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Top15 headlines sentiment analysis values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.52\n",
      "True negatives: 35\n",
      "True positives: 456\n",
      "False negatives: 43\n",
      "False positives: 511\n"
     ]
    }
   ],
   "source": [
    "x_train = reddit_train[['Top15 Sentiment']]\n",
    "x_test = reddit_test[['Top15 Sentiment']]\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('True negatives:', tn)\n",
    "print('True positives:', fp)\n",
    "print('False negatives:', fn)\n",
    "print('False positives:', tp)\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Top25 headlines sentiment analysis values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.53\n",
      "True negatives: 0\n",
      "True positives: 491\n",
      "False negatives: 0\n",
      "False positives: 554\n"
     ]
    }
   ],
   "source": [
    "x_train = reddit_train[['Top25 Sentiment']]\n",
    "x_test = reddit_test[['Top25 Sentiment']]\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('True negatives:', tn)\n",
    "print('True positives:', fp)\n",
    "print('False negatives:', fn)\n",
    "print('False positives:', tp)\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "So it appears that models that use the sentiment analysis values of articles of different popularity have roughly the same accuracy of predicting the DJIA. One other observation is that sentiment analysis doesn't appear to be a good feature at first glance. All models had an accuracy of 52-53%. Though maybe this is actually good for a single feature. As we know, predicting the stock market is hard and other features need to be considered before disregarding sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Are there better features we can use to predict the DJIA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes below, will be delted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use sentiment analysis on headlines with similar topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe sentiment analysis of the headlines isn't a good feature to use. ' Then use other features listed here: https://blog.quantinsti.com/machine-learning-logistic-regression-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe set a comparison using and then combine all the results like the cells below:\n",
    "x5_train = reddit_train[['Top5 Sentiment']]\n",
    "x5_test = reddit_test[['Top5 Sentiment']]\n",
    "\n",
    "y5_train = reddit_train[['Label']]\n",
    "y5_test = reddit_test[['Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top1 model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 model True negatives: 0\n",
      "Top1 model True positives: 491\n",
      "Top1 model False negatives: 0\n",
      "Top1 model False positives: 554\n",
      "[[  0 491]\n",
      " [  0 554]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Top1 model True negatives:', tn)\n",
    "print('Top1 model True positives:', fp)\n",
    "print('Top1 model False negatives:', fn)\n",
    "print('Top1 model False positives:', tp)\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
